# -*- coding: utf-8 -*-
"""Forager Finetune Distilbert

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g101Q83rLTXWUBOupS21a73RfW5qUNTt

# DistilBERT Fine-tuned for Foraging
Fine-tuning DistilBERT on the Forager Dataset with PyTorch Transformers on Google Colab.
"""
# flake8: noqa: E501

# Suppress warnings\
import schedule
import json
import time
import transformers
import datasets
import pandas as pd
import preprocessor as p
from datasets import Dataset
from transformers import AdamW, AutoTokenizer
from transformers import DataCollatorWithPadding
from torch.utils.data import DataLoader
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TFDistilBertForSequenceClassification, AutoModelForSequenceClassification, AutoConfig
from transformers import AutoModelForSequenceClassification
from torch.optim import AdamW
from transformers import get_scheduler
from tqdm.auto import tqdm
from datasets import load_metric
from numpy.ma.core import floor
import requests
from requests.structures import CaseInsensitiveDict
import numpy as np

# Load DistilBERT tokenizer and tokenize (encode) the texts
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")


def train():
    print(
        f"Running on transformers v{transformers.__version__} and datasets v{datasets.__version__}"
    )

    good = pd.read_csv(
        './dataset/good.txt',
        sep="Asdf12aaRdwsff",
        engine='python',
    )
    bad = pd.read_csv('./dataset/bad.txt',
                      sep="Asdf12aaRdwsff",
                      engine='python')
    df = pd.concat([good, bad], axis=1)

    df = df.stack().reset_index()
    df = df.drop(labels=['level_0'], axis=1)
    df = df.rename(columns={"level_1": 'labels', 0: 'text'})

    p.set_options(p.OPT.NUMBER, p.OPT.EMOJI)
    df['text'] = df['text'].apply(p.clean)

    print(df)
    # exit()

    # Put clean data in a dataset split into train and test sets
    dataset = Dataset.from_pandas(df).train_test_split(train_size=0.8,
                                                       seed=123)
    # print(dataset)

    # Cast labels column as class labels
    dataset = dataset.class_encode_column("labels")
    """## Tokenize data for DistilBERT"""

    # Make a list of columns to remove before tokenization
    cols_to_remove = [
        col for col in dataset["train"].column_names if col != "labels"
    ]

    # Tokenize and encode the dataset
    def tokenize(batch):
        tokenized_batch = tokenizer(
            batch['text'],  # tokenize the "text" column
            padding=True,  # 
            truncation=True,
            max_length=512)
        return tokenized_batch

    dataset_enc = dataset.map(tokenize,
                              batched=True,
                              remove_columns=cols_to_remove,
                              num_proc=4)

    # Set dataset format for PyTorch
    dataset_enc.set_format('torch',
                           columns=['input_ids', 'attention_mask', 'labels'])

    # Check the output
    print(dataset_enc["train"].column_names)

    # Instantiate a data collator with dynamic padding
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Create data loaders for to reshape data for PyTorch model
    train_dataloader = DataLoader(dataset_enc["train"],
                                  shuffle=True,
                                  batch_size=8,
                                  collate_fn=data_collator)
    eval_dataloader = DataLoader(dataset_enc["test"],
                                 batch_size=8,
                                 collate_fn=data_collator)
    """## Fine-tune DistilBERT"""

    # Dynamically set number of class labels based on dataset
    num_labels = dataset["train"].features["labels"].num_classes
    print(f"Number of labels: {num_labels}")

    # Load model from checkpoint
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased", num_labels=num_labels)

    # Model parameters
    learning_rate = 5e-5
    num_epochs = 15

    # Create the optimizer
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # Further define learning rate scheduler
    num_training_batches = len(train_dataloader)
    num_training_steps = num_epochs * num_training_batches
    lr_scheduler = get_scheduler(
        "linear",  # linear decay
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )

    # Set the device automatically (GPU or CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)

    # Move model to device
    model.to(device)

    progress_bar = tqdm(range(num_training_steps))

    # Train the model with PyTorch training loop
    model.train()
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

    # Save model to disk
    model.save_pretrained(f"./models/distilbert-forager")
    """## Evaluate model"""

    # Load metric
    metric = load_metric("glue", "mrpc")

    # Iteratively evaluate the model and compute metrics
    model.eval()
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])

    # Get model accuracy and F1 score
    metric.compute()

    # Convert PyTorch to TensorFlow checkpoint
    tf_model = TFDistilBertForSequenceClassification.from_pretrained(
        f"./models/distilbert-forager",
        # config=config,
        from_pt=True)

    # Save TensorFlow model to disk
    tf_model.save_pretrained(f"./models/distilbert-forager")


API_KEY = "h7u4huatuhfnbvz4knxjfhwapjfg6zjfbsiug37yhks90ruyhsjdhf723hu"
queue = []


def main():
    # train()
    """## Reload model from disk and inference"""

    # Set the device automatically (GPU or CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Define model config
    config = AutoConfig.from_pretrained(
        f"./models/distilbert-forager",
        # label2id=label2id,
        # id2label=id2label
    )

    # Load model from file and move to GPU
    model = AutoModelForSequenceClassification.from_pretrained(
        f"./models/distilbert-forager", config=config).to(device)

    def test_tweet():
        # Test tweet 1
        tweet_1 = ["""our credit card has been re-enabled"""]

        # Tokenize inputs
        start = time.time()
        inputs = tokenizer(tweet_1,
                           padding=True,
                           truncation=True,
                           return_tensors="pt").to(
                               device)  # Move the tensor to the GPU
        print(f"tokenizer() {floor((time.time() - start) / 1000)}ms")

        start = time.time()
        # Inference model and get logits
        outputs = model(**inputs)
        print(f"model() {(time.time() - start) / 1000}ms")

        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        print(predictions[0][0], predictions[0][1])

    # test_tweet()

    def listOfTuples(l1, l2):
        return list(map(lambda x, y: (x, y), l1, l2))

    def run_batch():
        headers = CaseInsensitiveDict()
        headers["Accept"] = "application/json"
        headers["Authorization"] = API_KEY

        response = requests.get(
            'https://forager.jeremyjacob.dev/api/scoring?skip=170000',
            headers=headers)
        data = response.json()
        # data = data[:5]

        print('Fetched data')
        snippets = [{'snippets': d['snippets'], '_id': d["_id"]} for d in data]
        flattened = [[{
            'snippet': item[:350],
            '_id': s['_id']
        } for item in s['snippets']] for s in snippets]
        flattened = np.concatenate(flattened).ravel().tolist()
        chunked_list = list()
        chunk_size = 10
        for i in range(0, len(flattened), chunk_size):
            chunked_list.append(flattened[i:i + chunk_size])

        for chunk in chunked_list:
            # Tokenize inputs
            inputs = tokenizer([c['snippet'] for c in chunk],
                               padding=True,
                               truncation=True,
                               return_tensors="pt").to(
                                   device)  # Move the tensor to the GPU

            # Inference model and get logits
            outputs = model(**inputs)

            # Convert logits to class probabilities
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            scores = [p[1].item() for p in predictions]
            # print(chunk, scores)
            merged = listOfTuples(chunk, scores)
            for (index, score) in enumerate(scores):
                chunk[index]['score'] = score
            queue.extend(chunk)

            # if scores:
            #     for tupl in merged:
            #         if tupl[1] > 0.6:
            #             # pass
            #             print(tupl)

    start = time.time()
    run_batch()
    print(f"Batch {time.time() - start}s")


def post():
    headers = CaseInsensitiveDict()
    headers["Accept"] = "application/json"
    headers["Authorization"] = API_KEY

    requests.post('https://forager.jeremyjacob.dev/api/scoring',
                  data=json.dumps(queue),
                  headers=headers)
    queue = []


if __name__ == '__main__':
    main()
    schedule.every(1).seconds.do(post)
    # schedule every second